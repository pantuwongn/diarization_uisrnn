{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A script showing how to DIARIZATION ON WAV USING UIS-RNN.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"A script showing how to DIARIZATION ON WAV USING UIS-RNN.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import uisrnn\n",
    "import librosa\n",
    "import sys\n",
    "sys.path.append('ghostvlad')\n",
    "sys.path.append('visualization')\n",
    "import toolkits\n",
    "import model as spkModel\n",
    "import os\n",
    "from viewer import PlotDiar\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable\n",
    "wav_filename = 'wavs/Maya.wav'\n",
    "SAVED_MODEL_NAME = 'pretrained/saved_model.uisrnn_benchmark'\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "global args\n",
    "# net choices=['resnet34s', 'resnet34l']\n",
    "# aggregation_mode choices=['avg', 'vlad', 'gvlad']\n",
    "# loss choices=['softmax', 'amsoftmax']\n",
    "# test_type  choices=['normal', 'hard', 'extend']\n",
    "args = Namespace( gpu='', resume=r'ghostvlad/pretrained/weights.h5', \n",
    "                  data_path='4persons', net='resnet34s', ghost_cluster=2,\n",
    "                  vlad_cluster=8, bottleneck_dim=512, aggregation_mode='gvlad',\n",
    "                  loss='softmax', test_type='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genMap(intervals):\n",
    "    ''' This function generate dictionary mapping from key frame and its interval\n",
    "            for each speech interval\n",
    "    '''\n",
    "    \n",
    "    # calculate interval for each slice\n",
    "    slicelen = [sliced[1]-sliced[0] for sliced in intervals.tolist()]\n",
    "    \n",
    "    # generate map table\n",
    "    mapTable = {}  \n",
    "    idx = 0\n",
    "    for i, sliced in enumerate(intervals.tolist()):\n",
    "        mapTable[idx] = sliced[0]\n",
    "        idx += slicelen[i]\n",
    "    mapTable[sum(slicelen)] = intervals[-1,-1]\n",
    "\n",
    "    # extract key(start) frame of each interval\n",
    "    keys = [k for k,_ in mapTable.items()]\n",
    "    keys.sort()\n",
    "    return mapTable, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append2dict(speakerSlice, spk_period):\n",
    "    ''' This helper function append spekaer period to the dict\n",
    "    '''\n",
    "    key = list(spk_period.keys())[0]\n",
    "    value = list(spk_period.values())[0]\n",
    "    timeDict = {}\n",
    "    timeDict['start'] = int(value[0]+0.5)\n",
    "    timeDict['stop'] = int(value[1]+0.5)\n",
    "    if(key in speakerSlice):\n",
    "        speakerSlice[key].append(timeDict)\n",
    "    else:\n",
    "        speakerSlice[key] = [timeDict]\n",
    "\n",
    "    return speakerSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangeResult(labels, time_spec_rate): \n",
    "    ''' This function arrance result of diarization to a dictionary\n",
    "        It maps speakerId to the list of interval information as example below\n",
    "        {'1': [{'start':10, 'stop':20}, {'start':30, 'stop':40}], '2': [{'start':90, 'stop':100}]}\n",
    "    '''\n",
    "    # get the first label, make it the last label to be considered\n",
    "    lastLabel = labels[0]\n",
    "    \n",
    "    # create empty output dictionary\n",
    "    speakerSlice = {}\n",
    "    \n",
    "    # for each loop, try to find the longest appearence of the label,\n",
    "    #   then, compute the time for that speech segment\n",
    "    j = 0\n",
    "    for i,label in enumerate(labels):\n",
    "        if(label==lastLabel):\n",
    "            continue\n",
    "        speakerSlice = append2dict(speakerSlice, {lastLabel: (time_spec_rate*j,time_spec_rate*i)})\n",
    "        j = i\n",
    "        lastLabel = label\n",
    "    speakerSlice = append2dict(speakerSlice, {lastLabel: (time_spec_rate*j,time_spec_rate*(len(labels)))})\n",
    "    \n",
    "    # return dict\n",
    "    return speakerSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0s        1s        2s                  4s                  6s\n",
    "# |-------------------|-------------------|-------------------|\n",
    "# |-------------------|\n",
    "#           |-------------------|\n",
    "#                     |-------------------|\n",
    "#                               |-------------------|\n",
    "def load_data(path, win_length=400, sr=16000, hop_length=160, n_fft=512, embedding_per_second=0.5, overlap_rate=0.5):\n",
    "    ''' This function reads wav file, and then create spectogram from the audio, finding the magnitude, frequency\n",
    "        Then, apply the sliding window to create spectogram information for each utterance\n",
    "    '''\n",
    "    \n",
    "    # read wav file\n",
    "    wav, intervals = load_wav(path, sr=sr)\n",
    "    \n",
    "    # create spectogram\n",
    "    linear_spect = lin_spectogram_from_wav(wav, hop_length, win_length, n_fft)\n",
    "    \n",
    "    # get magnitude of the spectograme\n",
    "    mag, _ = librosa.magphase(linear_spect)\n",
    "    \n",
    "    # get transpose of the magnitude numpy array\n",
    "    # we can get frequency, time\n",
    "    mag_T = mag.T\n",
    "    freq, time = mag_T.shape\n",
    "    spec_mag = mag_T\n",
    "\n",
    "    # calculate len of spectogram\n",
    "    spec_len = sr/hop_length/embedding_per_second\n",
    "    spec_hop_len = spec_len*(1-overlap_rate)\n",
    "\n",
    "    # prepare sliding window\n",
    "    cur_slide = 0.0\n",
    "    utterances_spec = []\n",
    "\n",
    "    # loop for each sliding window\n",
    "    while(True):\n",
    "        if(cur_slide + spec_len > time):\n",
    "            break\n",
    "            \n",
    "        # get spectogram magnitude of this sliding windiw\n",
    "        spec_mag = mag_T[:, int(cur_slide+0.5) : int(cur_slide+spec_len+0.5)]\n",
    "        \n",
    "        # preprocessing, subtract mean, divided by time-wise var\n",
    "        mu = np.mean(spec_mag, 0, keepdims=True)\n",
    "        std = np.std(spec_mag, 0, keepdims=True)\n",
    "        spec_mag = (spec_mag - mu) / (std + 1e-5)\n",
    "        \n",
    "        # store in teh uterance spectograme list\n",
    "        utterances_spec.append(spec_mag)\n",
    "\n",
    "        cur_slide += spec_hop_len\n",
    "\n",
    "    return utterances_spec, intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(vid_path, sr):\n",
    "    ''' This function loads wav file and convert to np array\n",
    "    '''\n",
    "    wav, _ = librosa.load(vid_path, sr=sr)\n",
    "    intervals = librosa.effects.split(wav, top_db=20)\n",
    "    wav_output = []\n",
    "    for sliced in intervals:\n",
    "        wav_output.extend(wav[sliced[0]:sliced[1]])\n",
    "    return np.array(wav_output), (intervals/sr*1000).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_spectogram_from_wav(wav, hop_length, win_length, n_fft=1024):\n",
    "    ''' This function get linear spectogram from wav file\n",
    "    '''\n",
    "    linear = librosa.stft(wav, n_fft=n_fft, win_length=win_length, hop_length=hop_length)\n",
    "    return linear.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmtTime(timeInMillisecond):\n",
    "    '''This helper functions convery time in millisecond to time string\n",
    "    '''\n",
    "    millisecond = timeInMillisecond%1000\n",
    "    minute = timeInMillisecond//1000//60\n",
    "    second = (timeInMillisecond-minute*60*1000)//1000\n",
    "    time = '{}:{:02d}.{}'.format(minute, second, millisecond)\n",
    "    return time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(wav_path, embedding_per_second=1.0, overlap_rate=0.5):\n",
    "    ''' This is the main function for do diarization using uis-rnn\n",
    "        The final result is influenced by the size of each window and the overlap rate.\n",
    "        When the overlap is too large, the uis-rnn perhaps generates fewer speakers \n",
    "            since the speaker embeddings changed smoothly, otherwise will generate more \n",
    "            speakers.\n",
    "        And also, the window size cannot be too short, it must contain enough information \n",
    "            to generate more discrimitive speaker embeddings.\n",
    "    '''\n",
    "\n",
    "    # gpu configuration\n",
    "    # if use gpu, specify device number \n",
    "    # otherswise, leave it blank\n",
    "    toolkits.initialize_GPU(args)\n",
    "\n",
    "    # parameters for uis-rnn\n",
    "    params = {'dim': (257, None, 1),\n",
    "              'nfft': 512,\n",
    "              'spec_len': 250,\n",
    "              'win_length': 400,\n",
    "              'hop_length': 160,\n",
    "              'n_classes': 5994,\n",
    "              'sampling_rate': 16000,\n",
    "              'normalize': True,\n",
    "              }\n",
    "\n",
    "    # create network and load weight\n",
    "    # this is for VGG-Speaker-recognition\n",
    "    network_eval = spkModel.vggvox_resnet2d_icassp(input_dim=params['dim'],\n",
    "                                                num_class=params['n_classes'],\n",
    "                                                mode='eval', args=args)\n",
    "    network_eval.load_weights(args.resume, by_name=True)\n",
    "    \n",
    "    # get model arguments for uis-rnn for speaker diarization\n",
    "    model_args, _, inference_args = uisrnn.parse_arguments()\n",
    "    model_args.observation_dim = 512\n",
    "    uisrnnModel = uisrnn.UISRNN(model_args)\n",
    "    uisrnnModel.load(SAVED_MODEL_NAME)\n",
    "\n",
    "    # load data and create spectogram\n",
    "    # specs: vector of spectogram\n",
    "    # intervals: frame period of the vector\n",
    "    specs, intervals = load_data(wav_path, embedding_per_second=embedding_per_second, overlap_rate=overlap_rate)\n",
    "    \n",
    "    # create dictionary for the previous information\n",
    "    # mapTable is the dictionary that map start frame to its interval\n",
    "    # keys is the list of the key in previous dictionary\n",
    "    mapTable, keys = genMap(intervals)\n",
    "\n",
    "    # create feature vector for each interval\n",
    "    feats = []\n",
    "    for spec in specs:\n",
    "        spec = np.expand_dims(np.expand_dims(spec, 0), -1)\n",
    "        v = network_eval.predict(spec)\n",
    "        feats += [v]\n",
    "\n",
    "    # feec feature vector to predict speaker for each interval\n",
    "    feats = np.array(feats)[:,0,:].astype(float)  # [splits, embedding dim]\n",
    "    predicted_label = uisrnnModel.predict(feats, inference_args)\n",
    "    \n",
    "    # arrange result to make the structure for speaker id, start, stop for each interval\n",
    "    time_spec_rate = 1000*(1.0/embedding_per_second)*(1.0-overlap_rate) # speaker embedding every ?ms\n",
    "    center_duration = int(1000*(1.0/embedding_per_second)//2)\n",
    "    speakerSlice = arrangeResult(predicted_label, time_spec_rate) # need to fix this to create json output\n",
    "\n",
    "    # create result dictionary\n",
    "    for spk,timeDicts in speakerSlice.items():    # time map to orgin wav(contains mute)\n",
    "        for tid,timeDict in enumerate(timeDicts):\n",
    "            s = 0\n",
    "            e = 0\n",
    "            for i,key in enumerate(keys):\n",
    "                if(s!=0 and e!=0):\n",
    "                    break\n",
    "                if(s==0 and key>timeDict['start']):\n",
    "                    offset = timeDict['start'] - keys[i-1]\n",
    "                    s = mapTable[keys[i-1]] + offset\n",
    "                if(e==0 and key>timeDict['stop']):\n",
    "                    offset = timeDict['stop'] - keys[i-1]\n",
    "                    e = mapTable[keys[i-1]] + offset\n",
    "\n",
    "            speakerSlice[spk][tid]['start'] = s\n",
    "            speakerSlice[spk][tid]['stop'] = e\n",
    "\n",
    "    # print out the result from the result dictionary\n",
    "    jsonDict = {}\n",
    "    for spk,timeDicts in speakerSlice.items():\n",
    "        print('========= Speaker ' + str(spk) + ' =========')\n",
    "        jsonDict['Speaker {}'.format(spk)] = []\n",
    "        for timeDict in timeDicts:\n",
    "            s = timeDict['start']\n",
    "            e = timeDict['stop']\n",
    "            s = fmtTime(s)  # change point moves to the center of the slice\n",
    "            e = fmtTime(e)\n",
    "            print(s+' ==> '+e)\n",
    "            jsonDict['Speaker {}'.format(spk)].append({'start':s, 'stop':e})\n",
    "    \n",
    "    with open('result.json', 'w') as fp:\n",
    "        json.dump(jsonDict, fp)\n",
    "\n",
    "    # plot the result of diarization on timeline\n",
    "    %matplotlib notebook\n",
    "    p = PlotDiar(map=speakerSlice, wav=wav_path, gui=True, size=(25, 6))\n",
    "    p.draw()\n",
    "    p.plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how to call\n",
    "main(wav_filename, embedding_per_second=1.2, overlap_rate=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spdi] *",
   "language": "python",
   "name": "conda-env-spdi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
